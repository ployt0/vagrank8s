# This is a basic workflow to help you get started with Actions

name: CI

on:
  push:
    branches: [ master ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: Install Virtualbox
        run: |
          sudo apt-get update
          sudo apt-get install virtualbox
          pwd

      - name: Install Vagrant
        run: |
          curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
          sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
          sudo apt-get update && sudo apt-get install vagrant
          vagrant --version

      - name: Start Control-Plane
        run: |
          cd master
          vagrant up

      - name: Start Node1
        run: |
          cd ../node1
          vagrant up

      - name: Start Node2
        run: |
          cd ../node2
          vagrant up

      - name: Create echo service
        run: |
          cd ../master
          vagrant ssh -c "kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.10 --replicas=2 && \
                kubectl expose deployment hello-node --type=LoadBalancer --port=8080"

      - name: Test echo service from master
        run: |
          # Wait for pods to be ready.
          # sleep 1
          vagrant ssh -c "pod1_ip=\$(kubectl get pod -l app=hello-node -o=custom-columns=IP:.status.podIP | sed -n '2 p') && \
              pod2_ip=\$(kubectl get pod -l app=hello-node -o=custom-columns=IP:.status.podIP | sed -n '3 p') && \
              svc_ip=\$(kubectl get svc hello-node -o=custom-columns=CLUSTER-IP:.spec.clusterIP | tail -n 1) && \
              echo "pod1_ip=\$pod1_ip"
              echo "pod2_ip=\$pod2_ip"
              echo "svc_ip=\$svc_ip"
              ping -c3 -W2 \$pod1_ip && \
              ping -c3 -W2 \$pod2_ip && \
              host_collection='' && \
              for i in {1..20}; do \
                hosty=(\$(curl -s \$svc_ip:8080 | grep Hostname)) && \
                host_collection+=\" \${hosty[1]}\"; \
              done; \
              echo \$host_collection | tr ' ' '\n' | sort -u | [ \$(wc -l) -eq 2 ] && echo PASS || echo FAIL" > results.txt
          cat results.txt
          cat results.txt | grep pod1_ip= >> $GITHUB_ENV
          cat results.txt | grep pod2_ip= >> $GITHUB_ENV
          cat results.txt | grep svc_ip= >> $GITHUB_ENV
          tail -n1 results.txt | grep "PASS"

      - name: Test echo service from node1
        run: |
          cd ../node1
          vagrant ssh -c "ping -c3 -W2 $pod1_ip && \
              ping -c3 -W2 $pod2_ip && \
              host_collection='' && \
              for i in {1..20}; do \
                hosty=(\$(curl -s $svc_ip:8080 | grep Hostname)) && \
                host_collection+=\" \${hosty[1]}\"; \
              done; \
              echo \$host_collection | tr ' ' '\n' | sort -u | [ \$(wc -l) -eq 2 ] && echo PASS || echo FAIL" > results.txt
          cat results.txt
          tail -n1 results.txt | grep "PASS"

      - name: Test echo service from node2
        run: |
          cd ../node2
          vagrant ssh -c "ping -c3 -W2 $pod1_ip && \
              ping -c3 -W2 $pod2_ip && \
              host_collection='' && \
              for i in {1..20}; do \
                hosty=(\$(curl -s $svc_ip:8080 | grep Hostname)) && \
                host_collection+=\" \${hosty[1]}\"; \
              done; \
              echo \$host_collection | tr ' ' '\n' | sort -u | [ \$(wc -l) -eq 2 ] && echo PASS || echo FAIL" > results.txt
          cat results.txt
          tail -n1 results.txt | grep "PASS"

      - name: CD to master
        run: |
          cd ../master

      - name: Exceed Host Network Limitation (node count)
        run: |
          vagrant ssh -c "for i in {1..3}; do \
            cat <<<-EOF | kubectl create -f -
              apiVersion: v1
              kind: Pod
              metadata:
                name: shell-demo\$i
              spec:
                volumes:
                - name: shared-data
                  emptyDir: {}
                containers:
                - name: nginx
                  image: nginx
                  volumeMounts:
                  - name: shared-data
                    mountPath: /usr/share/nginx/html
                hostNetwork: true
                dnsPolicy: Default
            EOF
          done"

      - name: Leave one shell-demo pod running
        run: |
          vagrant ssh -c "for i in {2..3}; do \
            kubectl delete pod shell-demo\$i
          done"

      - name: Test echo service from pod inside cluster
        run: |
          vagrant ssh -c "kubectl exec -ti shell-demo1 -- /bin/bash -c \"curl $svc_ip:8080 && apt-get update && apt-get install iputils-ping -y\""
          vagrant ssh -c "kubectl exec -ti shell-demo1 -- /bin/bash -c \" \
              ping -c3 -W2 $pod1_ip && \
              ping -c3 -W2 $pod2_ip && \
              host_collection='' && \
              for i in {1..10}; do \
                hosty=(\\\$(curl -s $svc_ip:8080 | grep Hostname)) && \
                host_collection+=\\\" \\\${hosty[1]}\\\"; \
              done;
              echo \\\$host_collection | tr ' ' '\n' | sort -u | [ \\\$(wc -l) -eq 2 ] && echo PASS || echo FAIL\"" > results.txt
          cat results.txt
          tail -n1 results.txt | grep "PASS"

